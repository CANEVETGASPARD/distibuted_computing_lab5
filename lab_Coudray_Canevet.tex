\documentclass[11pt]{article}

\usepackage{sectsty}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage[dvipsnames]{xcolor}

% Margins
\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\title{Lab 5: Voting-based probabilistic consensuses}
\author{CANEVET Gaspard, COUDRAY Amaury}
\date{November 16, 2022}

\begin{document}
\maketitle	

\vspace{10mm}

% Optional TOC
% \tableofcontents
% \pagebreak

%--Paper--

\section{Majority dynamics model}

The system consists of n nodes. At each instant $k \in IN^{+}$ :
\begin{enumerate}
    \item One node is selected uniformly at random.
    \item The selected node then chooses three nodes independently and uniformly at random. In particular, it may choose itself, and may choose some nodes more than once (draw with replacement).
    \item The selected node then adopts the majority opinion of the chosen nodes.
\end{enumerate}

\noindent We denote by $X_{k}$ the number of nodes with opinion 1 at time k.

\vspace{5mm}

\noindent 1/ For every $m, m' \in \{0, . . . , n\}$, can you describe the probability that $X_{k+1} = m'$ knowing that $X_{k} = m$? 

\vspace{5mm}

\noindent There, we are looking for $P[X_{k+1} = m'|X_{k} = m]$. \\
- Following the above algorithm we can define two border cases $m=0$ and $m=n$. In both cases, a consensus is found and every nodes have the same opinion.
In those cases we have $m'=m$ and $P[X_{k+1} = m'|X_{k} = m] = 1$ \\
- Then a more global point of view give us the bellow three possibilities for m': 
\begin{enumerate}[topsep=0pt, itemsep=0pt]
    \item m' = m-1 if we pull out a node with opinion 1 at the first step and at least two nodes with opinion 0 at the second step.
    \item m' = m if we pull out a node with opinion 1 or 0 at the first step and at least two nodes with the same opinion at the second step.
    \item m' = m+1 if we pull out a node with opinion 0 at the first step and at least two nodes with opinion 1 at the second step.
\end{enumerate}

\vspace{5mm}

\noindent Now let's try to solve every parts.

(1) $P[X_{k+1} = m-1|X_{k} = m]$ = P[pull out a node with opinion 1 at step 1]*P[pull out two nodes with opinion 0 at step 2]. \\
Thus $P[X_{k+1} = m-1|X_{k} = m]$ = $\frac{m}{n}*(\frac{n-m}{n})^{2}$ (keep in mind that we have m nodes with opinion 1 and n-m nodes with opinion 0)

(2) $P[X_{k+1} = m+1|X_{k} = m]$ = P[pull out a node with opinion 0 at step 1]*P[pull out two nodes with opinion 1 at step 2]. \\
Thus $P[X_{k+1} = m+1|X_{k} = m]$ = $\frac{n-m}{n}*(\frac{m}{n})^{2}$

(3) $P[X_{k+1} = m|X_{k} = m]$ = 1 - ($P[X_{k+1} = m+1|X_{k} = m]$ + $P[X_{k+1} = m-1|X_{k} = m]$)
Thus $P[X_{k+1} = m|X_{k} = m]$ = 1 - ($\frac{n-m}{n}*(\frac{m}{n})^{2}$ + $\frac{m}{n}*(\frac{n-m}{n})^{2}$)

\vspace{5mm}

\noindent Finally we have: 

\begin{equation}
    P[X_{k+1} = m'|X_{k} = m]=
    \begin{cases}
        1 & \text{if m=n or 0}\\
        \frac{m}{n}*(\frac{n-m}{n})^{2} & \text{if m'=m-1}\\
        \frac{n-m}{n}*(\frac{m}{n})^{2} & \text{if m'=m+1}\\
        1 - (\frac{n-m}{n}*(\frac{m}{n})^{2} + \frac{m}{n}*(\frac{n-m}{n})^{2}) & \text{if m'=m}\\
        0 & else

    \end{cases}
\end{equation}


\begin{center}
    \noindent\rule{8cm}{0.4pt} 
\end{center}

\vspace{5mm}

\noindent 2/ What are the absorbing states of this Markov chain? What can you conclude?

\vspace{5mm}

\noindent The absorbing points of this Markov chain are m=n or 0. In those cases, we already have found a concensus. for example, if we have m=n, we will pull out a node with opinion 1 at step 1 and three nodes with opinion 1 at step two. Thus the third step will not change the state of the pulled node.


\begin{center}
    \noindent\rule{8cm}{0.4pt} 
\end{center}


\noindent Let $\tau_{A}$ := min$\{k \geq 1 : X_{n} \in A\}$ denote the hitting time of the set A. We will also use the
notations $P_{x}$ and $E_{x}$ for the probability and expectations with respect to the process started at x. The
following theorem has been proved:
\begin{center}
    \colorbox{Gray}{\textbf{Lemma:} For all x $\in$ \{1, . . . , n-1\}, we have that: $E_{x}[\tau_{0,n}] \leq \frac{256}{15}n*(1 + log(n))$}
\end{center}

\vspace{5mm}

\noindent 3/ Can you interpret the result of this lemma? What is it telling in term of performance?

\vspace{5mm}

\noindent This lemma tells us that in expectation the algorithm will converge in nlog(n).

\begin{center}
    \noindent\rule{8cm}{0.4pt} 
\end{center}

We assume now the presence in the system of Byzantine nodes. Let q $\in$ (0, 12) be the fraction
of Byzantine nodes in the system. Therefore, among the n nodes, (1 - q)*n are honest (i.e., they follow
the prescribed protocol), and q*n are Byzantine.

\vspace{5mm}

\noindent 4-1/ We will assume that Byzantine nodes are controlled by a single entity, called the adversary. We
also assume that this entity knows the current state of the system, in other words, the adversary
is omniscient. Do you think this assumption is reasonable, or not? Justify your answer. 

\vspace{5mm}

\noindent This assumption is reasonable because the adversary is the one who controls the Byzantine nodes. Thus, he knows the current state of the system.

\vspace{5mm}

\noindent 4-2/ An adversary can have different strategies to modify the output of the voting mechanism. Can
you give three possible strategies and explain their effects on the final result of the algorithm?

\vspace{5mm}

\noindent The adversary can have three different strategies:
\begin{itemize}
    \item always the same opinion: the adversary can always give the same opinion to all the honest nodes.
    \item random opinion: the adversary can give a random opinion to all the honest nodes.
    \item majority opinion: the adversary can give the majority opinion to all the honest nodes.
\end{itemize}

\vspace{5mm}

\noindent 4-3/ Now let us consider the following adversarial strategy: ”Help the weakest”. This strategy is
working as follow: the adversarial nodes will always vote on the less popular opinion among
the honest nodes. Let $\overline{X_{k}}$ be the number of honest nodes with opinion 1 at instant k. Do you
think that 0 and 1 are still absorbing states? Can you describe the transition probability of this
Markov chain, $\overline{X_{k}}$ = m.

\vspace{5mm}

\noindent 0 and n are not absorbing states anymore. For instance, let's assume that m=n, we can pull out a non bynzantine node at step 1 
with opinion 1 and two byzantine nodes at step two. The two byzantine nodes will provide a message with opinion 0 and thus 
will change the opinion of the nodes. \\
The transition probability of this Markov chain is:

si $m < n/2$
\begin{equation}
    P[\overline{X_{k+1}} = m'|\overline{X_{k}} = m]=
    \begin{cases}
        \frac{m}{n}*(\frac{(1-q)*n-m}{n})^{2} & \text{if m'=m-1}\\
        \frac{((1-q)*n-m)}{n}*(\frac{m+ q*n}{n})^{2} & \text{if m'=m+1}\\
        1 - (\frac{m}{n}*(\frac{(1-q)*n-m}{n})^{2} + \frac{((1-q)*n-m)}{n}*(\frac{m+ q*n}{n})^{2}) & \text{if m'=m}\\
        0 & else
    \end{cases}
\end{equation}

si $m > n/2$
\begin{equation}
    P[\overline{X_{k+1}} = m'|\overline{X_{k}} = m]=
    \begin{cases}
        \frac{m}{n}*(\frac{n-m}{n})^{2} & \text{if m'=m-1}\\
        \frac{(1-q)*n-m}{n}*(\frac{m}{n})^{2} & \text{if m'=m+1}\\
        1 - (\frac{m}{n}*(\frac{n-m}{n})^{2} + \frac{(1-q)*n-m}{n}*(\frac{m}{n})^{2}) & \text{if m'=m}\\
        0 & else
    \end{cases}
\end{equation}




\vspace{10mm}

\section{Implementations of the fast probabilistic consensus}

\vspace{5mm}

\noindent 5/ What can be a simple attack on the fast probabilistic consensus algorithm proposed
by Serguei Popov and William J. Buchanan

\vspace{5mm}

\noindent U(k) is determined by the random variable U, so the Byzantine node can forecast the value of u(k) and then send a message to prevent the algorithm from converging.

\vspace{5mm}

\noindent 6/ Implement in Go the FPC with the following requirements:
\begin{itemize}
    \item The network is composed of five nodes.
    \item Node 1 is a Byzantine node. It will use the ”Help the weakest” strategy.
    \item You need measure the time taken by the algorithm to stop and wether or not the algorithm has
    reached the desired consensus.
\end{itemize}

\vspace{5mm}

\noindent See code in src folder. We set $\nu$ to 20 and $\beta$ to 0.2. Without byzantine nodes we converge in 30 interations, roughly speaking.
With byzantine nodes, the biggest convergence value we have seen was 150k iterations.



%--/Paper--

\end{document}